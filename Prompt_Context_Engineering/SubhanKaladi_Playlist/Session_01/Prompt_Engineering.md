# **What is Prompt Engineering?**

Prompt engineering is the art and science of crafting instructions that guide AI language models to produce desired outputs. Think of it as learning to communicate effectively with AI systems to achieve specific goals.

## **Why is it important?**

* You don't need to be a programmer to use AI effectively
* Good prompts can dramatically improve AI performance. Clear prompts reduce misinterpretation, ensuring AI outputs align with user intent.
* It's an iterative skill that improves with practice
* It's becoming essential for productivity in many fields

**Example**

```bash
Bad Prompt:
"Write about space."

Good Prompt:
"You are an astrophysicist. Write a 200-word summary explaining black holes to 10-year-olds, using analogies and simple language."

```

## **Prompt engineering vs. context engineering**

**Prompt engineering** = crafting the instruction you give the model. 
**Context engineering** = curating the information the model can see when following that instruction.

### Quick contrast

| Aspect          | Prompt engineering                                                       | Context engineering                                                                      |
| --------------- | ------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------- |
| Goal            | Tell the model *how* to behave and *what* to produce                     | Give the model the *facts/examples* it should rely on                                    |
| Levers          | Wording, structure, roles, constraints, output schema, few-shot examples | Retrieval (RAG), documents, knowledge bases, tools/APIs, memory, state across turns      |
| Typical changes | ‚ÄúBe concise. Return valid JSON with fields X/Y/Z.‚Äù                       | ‚ÄúAttach the company glossary, latest policy PDF, and retrieved passages for this query.‚Äù |
| Failure mode    | Vague instructions ‚Üí messy/incorrect format                              | Missing/irrelevant info ‚Üí hallucinations/outdated answers                                |
| Ownership       | UX/prompt designers, app devs                                            | Data/ML/platform teams (pipelines, indexing, chunking, evals)                            |

- In Ownership section of above table, pipelines, indexing, chunking and evals are mentioned in `Context Engineering` column, **what they are?**

1) **#### Pipelines**

- **Meaning:** A pipeline is the *sequence of automated steps* that process data before it reaches the AI model.
- **Purpose:** To prepare, clean, and structure data so it can be used efficiently as context or training material.
- **Example in AI agents:** When a user uploads a company document:
    1) The pipeline reads the document (PDF, HTML, etc.)
    2) Cleans and formats the text
    3) Splits it into smaller chunks
    4) Embeds each chunk (turns it into vectors)
    5) Saves those vectors to a database for retrieval later

So pipelines are like **assembly lines** for data ‚Äî turning raw information into context the model can use.

2) **#### Indexing**

- **Meaning:** Indexing means organizing data in a way that makes it *fast and efficient to search or retrieve later.*
- **In context engineering:** After embedding text chunks (turning them into numerical vectors), you **store them in a ‚Äúvector index‚Äù** ‚Äî a searchable database (like Pinecone, Weaviate, or Supabase Vector).
- **Example:** When a user asks: ‚ÄúWhat‚Äôs our refund policy?‚Äù
    * The agent searches the **index** to quickly find the most relevant policy passages
    * Then passes those passages as *context* to the LLM

So indexing = **searchable memory** for your AI agent.

3) **#### Chunking**

- **Meaning:** Chunking means breaking large pieces of text (documents, articles, etc.) into *smaller, meaningful segments (‚Äúchunks‚Äù)* before embedding or retrieval.
- **Why?** LLMs have limited context windows ‚Äî they can‚Äôt read entire books or databases at once.
So you divide data into digestible pieces that still preserve meaning.
- **Example:** A 20-page policy document ‚Üí split into 200‚Äì500-word chunks, each stored separately in the **vector database**.

That way, when a user asks a question, the AI retrieves *just the relevant chunks* instead of reading everything.

4) **#### Evaluations**

- **Meaning:** Evals are tests or metrics used to measure how well your AI system (prompts + context pipeline) performs.
- **Purpose:** To check if your agent gives accurate, relevant, and consistent answers ‚Äî and to improve it over time.
- **Example:** 
    * You might run 100 test queries like:
        - ‚ÄúWhat is the refund policy?‚Äù
        - ‚ÄúHow do I reset my password?‚Äù
    * Then evaluate:
        - ‚úÖ Accuracy of retrieved info
        - üß© Relevance of context
        - üí¨ Quality of final LLM answer

Evals = **quality control** for your AI pipeline.

### **How they work together**

* Start with a **good prompt**: clear task, constraints, and an **output contract** (e.g., JSON schema).
* Then **ground it with context**: supply only the *most relevant* passages, tables, and tool results.
* The prompt guides *behavior*; the context supplies *knowledge*. You usually need both.

### **Concrete examples**

1. **Invoice ‚Üí JSON extractor**

* *Prompt engineering*: ‚ÄúExtract fields {vendor, date, total}. Return JSON only. If a field is missing, use null.‚Äù
* *Context engineering*: Provide a few labeled examples and attach the vendor‚Äôs invoice spec retrieved via embeddings.

2. **Policy Q\&A bot**

* *Prompt engineering*: ‚ÄúAnswer using the attached passages; if unsure, say ‚ÄòNot in policy.‚Äô Cite section IDs.‚Äù
* *Context engineering*: RAG over your policy repo (chunking, metadata filters like `department=HR`, freshness boosts), plus a recency cache for updates.

3. **Agentic workflow**

* *Prompt engineering*: Tool-use instructions and function signatures.
* *Context engineering*: Feed tool responses (DB rows, API payloads) back into the context window each step; maintain short-term memory/state.

### **Practical tips**

* Keep prompts **short, specific, and testable**; define output schemas.
* Prefer **few-shot** examples only when they generalize; otherwise move them into **retrieval**.
* For context: optimize **chunking**, **ranking**, **deduping**, and **token budgets**; log what was retrieved for each answer.
* Add **citations** and ‚Äúanswer only from context‚Äù instructions when correctness matters.
* Evaluate both layers separately: prompt A/B tests and retrieval quality (precision/recall, groundedness).

**One-liner:** Prompt engineering is *how you ask*; context engineering is *what you show*. Combine them for reliable, scalable LLM apps.